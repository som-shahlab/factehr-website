<div align="center">
  <h1>ğŸ“„ ğŸ§  FactEHR</h1>
  <h4>
    <a href="https://stanford.redivis.com/datasets/bckk-15p0mwmz7">ğŸ’¾ Dataset</a> â€¢ 
    <a href="https://arxiv.org/abs/2412.12422">ğŸ“ Paper</a> â€¢ 
    <a href="https://github.com/som-shahlab/factehr">âš™ï¸ Code & Docs</a>
  </h4>
  <h4>A benchmark for fact decomposition and entailment evaluation of clinical notes</h4>
  <p>
    2,168 notes â€¢ 8,665 decompositions â€¢ 987,266 entailment pairs â€¢ 1,036 human-labeled examples
  </p>
</div>

---

# ğŸ§  FactEHR: A Benchmark for Fact Decomposition of Clinical Notes

**FactEHR** is a benchmark dataset designed to evaluate the ability of large language models (LLMs) to perform **factual reasoning** over clinical notes. It includes:

- **2,168** deidentified notes from multiple publicly available datasets  
- **8,665** LLM-generated fact decompositions  
- **987,266** entailment pairs evaluating precision and recall of facts  
- **1,036** expert-annotated examples for evaluation

FactEHR supports LLM evaluation across tasks like **information extraction**, **entailment classification**, and **model-as-a-judge** reasoning.

> [!WARNING]  
> **Usage Restrictions:** The FactEHR dataset is subject to a Stanford Dataset DUA. Sharing data with LLM API providers is prohibited.  
> We follow PhysioNetâ€™s responsible use principles for running LLMs on sensitive clinical data:
> 
> - âœ… Use **Azure OpenAI** (with human review opt-out)  
> - âœ… Use **Amazon Bedrock** (private copies of foundation models)  
> - âœ… Use **Google Gemini via Vertex AI** (non-training usage)  
> - âœ… Use **Anthropic Claude** (no prompt data used for training)
> - âŒ **Do not transmit data** to commercial APIs (e.g., ChatGPT, Gemini, Claude) unless HIPAA-compliant and explicitly permitted  
> - âŒ **Do not share** notes or derived outputs with third parties

---

## ğŸ“¦ What's Included

| Component          | Count     | Description                                               |
|-------------------|-----------|-----------------------------------------------------------|
| Clinical Notes     | 2,168     | Deidentified clinical notes across 4 public datasets      |
| Fact Decompositions | 8,665     | Model-generated fact lists from each note                |
| Entailment Pairs   | 987,266   | Pairs evaluating if notes imply facts (and vice versa)   |
| Expert Labels      | 1,036     | Human-annotated entailment labels for benchmarking        |

See the [data summary](docs/dataset_summary.md) and [release files](docs/release_files.md) for more details.

---

## ğŸ› ï¸ Installation

```bash
python -m pip install -e .
```
---

## ğŸ§ª Running the Experiments

We support two core experiments for evaluating factual reasoning in clinical notes:

### 1. ğŸ§© Fact Decomposition

This task involves prompting LLMs to extract structured atomic facts from raw clinical notes.

**Inputs:**
- Notes from `combined_notes_110424.csv`
- Prompt templates in `prompts/`
- An LLM provider (e.g., OpenAI, Claude, Bedrock)

**Outputs:**
- A list of decomposed facts per note (stored in `fact_decompositions_*.csv`)

See [docs/experiments.md](docs/experiments.md#1-generating-fact-decompositions) for instructions on:
- Supported prompt formats
- Batch processing with rate-limited APIs
- Handling invalid or unparseable outputs

---

### 2. ğŸ” Entailment Evaluation

FactEHR supports two entailment settings:
- **Precision (`note â‡’ fact`)** â€” Does a note imply a given fact?
- **Recall (`fact â‡’ sentence`)** â€” Does a fact imply a sentence in the note?

**Approaches:**
- Use your own classifier or fine-tuned entailment model
- Use an LLM-as-a-judge (e.g., GPT-4, Claude) to score entailment pairs

**Inputs:**
- Entailment pairs in `entailment_pairs_110424.csv`
- Fact decompositions and source notes
- Optional: human-labeled samples for evaluation

**Outputs:**
- Entailment predictions (binary labels or probabilities)
- Comparison against human annotations for calibration

See [docs/experiments.md](docs/experiments.md#2-running-llm-experiments) for:
- Prompting logic
- Suggested evaluation metrics
- Example LLM judge scripts

---


## ğŸ“š Citation

If you use **FactEHR** in your research, please cite:

```bibtex
@article{munnangi2024factehr,
  title     = {Assessing the Limitations of Large Language Models in Clinical Fact Decomposition},
  author    = {Monica Munnangi and Akshay Swaminathan and Jason Alan Fries and Jenelle Jindal and Sanjana Narayanan and Ivan Lopez and Lucia Tu and Philip Chung and Jesutofunmi A. Omiye and Mehr Kashyap and Nigam Shah},
  journal   = {arXiv preprint arXiv:2412.12422},
  year      = {2024},
  eprint    = {2412.12422},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  note      = {v1, 17 Dec 2024}
}
